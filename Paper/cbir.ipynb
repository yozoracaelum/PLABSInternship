{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBIR Dataset\n",
    "https://www.kaggle.com/datasets/theaayushbajaj/cbir-dataset/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from barbar import Bar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gc\n",
    "RANDOMSTATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/cbir-dataset/dataset/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/cbir-dataset/dataset/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/cbir-dataset/dataset/10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/cbir-dataset/dataset/100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/cbir-dataset/dataset/1000.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         image\n",
       "0     /kaggle/input/cbir-dataset/dataset/0.jpg\n",
       "1     /kaggle/input/cbir-dataset/dataset/1.jpg\n",
       "2    /kaggle/input/cbir-dataset/dataset/10.jpg\n",
       "3   /kaggle/input/cbir-dataset/dataset/100.jpg\n",
       "4  /kaggle/input/cbir-dataset/dataset/1000.jpg"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing intermediate DataFrame\n",
    "datasetPath = Path('dataset/dataset/')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n",
    "df['image'] = '/kaggle/input/cbir-dataset/dataset/' + df['image'].astype(str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIRDataset(Dataset):\n",
    "    def __init__(self, dataFrame):\n",
    "        self.dataFrame = dataFrame\n",
    "        \n",
    "        self.transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            raise NotImplementedError('slicing is not supported')\n",
    "        \n",
    "        row = self.dataFrame.iloc[key]\n",
    "        image = self.transformations(Image.open(row['image']))\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataFrame.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Function to process data from the data retrival class\n",
    "def prepare_data(DF):\n",
    "    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n",
    "    train_set = CBIRDataset(trainDF)\n",
    "    validate_set = CBIRDataset(validateDF)\n",
    "    \n",
    "    return train_set, validate_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=3, padding=1),  # (32,16,171,171)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride=2, padding=1),  # (N,8,43,43)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 8, out_channels=16, kernel_size=(3,3), stride=2),  # (N,16,85,85)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=(5,5), stride=3, padding=1),  # (N,8,255,255)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=(6,6), stride=2, padding=1),  # (N,3,512,512)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder_v2, self).__init__()\n",
    "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), stride=1, padding=0), \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=2, padding=1), \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding=1), \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding=1), \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 256, out_channels=256, kernel_size=(3,3), stride=1, padding=1), \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3,3), stride=2, padding=0),  \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(3,3), stride=2, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(3,3), stride=2, padding=1), \n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=2, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=(4,4), stride=2, padding=2),  \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "              ReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3         [-1, 64, 512, 512]          36,928\n",
      "              ReLU-4         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-5         [-1, 64, 256, 256]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 126, 126]         147,584\n",
      "              ReLU-9        [-1, 128, 126, 126]               0\n",
      "        MaxPool2d-10          [-1, 128, 63, 63]               0\n",
      "           Conv2d-11          [-1, 256, 32, 32]         295,168\n",
      "             ReLU-12          [-1, 256, 32, 32]               0\n",
      "           Conv2d-13          [-1, 256, 32, 32]         590,080\n",
      "             ReLU-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
      "             ReLU-16          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-17          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-18          [-1, 256, 16, 16]         590,080\n",
      "  ConvTranspose2d-19          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-20          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-21          [-1, 128, 33, 33]         295,040\n",
      "  ConvTranspose2d-22           [-1, 64, 65, 65]          73,792\n",
      "             ReLU-23           [-1, 64, 65, 65]               0\n",
      "  ConvTranspose2d-24         [-1, 32, 129, 129]          18,464\n",
      "  ConvTranspose2d-25         [-1, 32, 257, 257]           9,248\n",
      "             ReLU-26         [-1, 32, 257, 257]               0\n",
      "  ConvTranspose2d-27          [-1, 3, 512, 512]           1,539\n",
      "             Tanh-28          [-1, 3, 512, 512]               0\n",
      "================================================================\n",
      "Total params: 3,313,731\n",
      "Trainable params: 3,313,731\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 678.39\n",
      "Params size (MB): 12.64\n",
      "Estimated Total Size (MB): 694.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ConvAutoencoder_v2().to(device),(3,512,512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
    "    \n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    #valid_loss_min = checkpoint['valid_loss_min']\n",
    "\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    print (\"=> Saving a new best\")\n",
    "    torch.save(state, filename)  # save checkpoint\n",
    "    \n",
    "def train_model(model,  \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                #scheduler, \n",
    "                num_epochs):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                save_checkpoint(state={   \n",
    "                                    'epoch': epoch,\n",
    "                                    'state_dict': model.state_dict(),\n",
    "                                    'best_loss': best_loss,\n",
    "                                    'optimizer_state_dict':optimizer.state_dict()\n",
    "                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "NUM_BATCHES = 128\n",
    "RETRAIN = False\n",
    "\n",
    "train_set, validate_set = prepare_data(DF=df)\n",
    "\n",
    "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n",
    "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n",
    "                }\n",
    "\n",
    "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
    "\n",
    "model = ConvAutoencoder_v2().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If re-training is required:\n",
    "# Load the old model\n",
    "if RETRAIN == True:\n",
    "    # load the saved checkpoint\n",
    "    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n",
    "    print('Checkpoint Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, loss = train_model(model=model, criterion=criterion, optimizer=optimizer, \n",
    "                    #scheduler=exp_lr_scheduler,\n",
    "                    num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "torch.save({\n",
    "            'epoch': EPOCHS,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'conv_autoencoderv2_200ep.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### 1. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model in Evaluation phase\n",
    "model = ConvAutoencoder_v2().to(device)\n",
    "model.load_state_dict(torch.load('conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_features(images, transformations):\n",
    "    \n",
    "    latent_features = np.zeros((4738,256,16,16))\n",
    "    #latent_features = np.zeros((4738,8,42,42))\n",
    "    \n",
    "    for i,image in enumerate(tqdm(images)):\n",
    "        tensor = transformations(Image.open(image)).to(device)\n",
    "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
    "        \n",
    "    del tensor\n",
    "    gc.collect()\n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df.image.values\n",
    "latent_features = get_latent_features(images, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(0, 4738))\n",
    "feature_dict = dict(zip(indexes,latent_features))\n",
    "index_dict = {'indexes':indexes,'features':latent_features}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(a, b):\n",
    "    # compute and return the euclidean distance between two vectors\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a,b):\n",
    "    return scipy.spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(queryFeatures, index, maxResults=64):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(index[\"features\"])):\n",
    "        # compute the euclidean distance between our query features\n",
    "        # and the features for the current image in our index, then\n",
    "        # update our results list with a 2-tuple consisting of the\n",
    "        # computed distance and the index of the image\n",
    "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
    "        results.append((d, i))\n",
    "    \n",
    "    # sort the results and grab the top ones\n",
    "    results = sorted(results)[:maxResults]\n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_montages(image_list, image_shape, montage_shape):\n",
    "\n",
    "    if len(image_shape) != 2:\n",
    "        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n",
    "    if len(montage_shape) != 2:\n",
    "        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n",
    "    image_montages = []\n",
    "    # start with black canvas to draw images onto\n",
    "    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
    "                          dtype=np.uint8)\n",
    "    cursor_pos = [0, 0]\n",
    "    start_new_img = False\n",
    "    for img in image_list:\n",
    "        if type(img).__module__ != np.__name__:\n",
    "            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n",
    "        start_new_img = False\n",
    "        img = cv2.resize(img, image_shape)\n",
    "        # draw image to black canvas\n",
    "        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n",
    "        cursor_pos[0] += image_shape[0]  # increment cursor x position\n",
    "        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n",
    "            cursor_pos[1] += image_shape[1]  # increment cursor y position\n",
    "            cursor_pos[0] = 0\n",
    "            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n",
    "                cursor_pos = [0, 0]\n",
    "                image_montages.append(montage_image)\n",
    "                # reset black canvas\n",
    "                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
    "                                      dtype=np.uint8)\n",
    "                start_new_img = True\n",
    "    if start_new_img is False:\n",
    "        image_montages.append(montage_image)  # add unfinished montage\n",
    "    return image_montages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the features for the current image, find all similar\n",
    "# images in our dataset, and then initialize our list of result\n",
    "# images\n",
    "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
    "queryIdx = 3166 # Input Index for which images \n",
    "MAX_RESULTS = 10\n",
    "\n",
    "queryFeatures = latent_features[queryIdx]\n",
    "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
    "imgs = []\n",
    "\n",
    "# loop over the results\n",
    "for (d, j) in results:\n",
    "    img = np.array(Image.open(images[j]))\n",
    "    print(j)\n",
    "    imgs.append(img)\n",
    "\n",
    "# display the query image\n",
    "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
    "\n",
    "# build a montage from the results and display it\n",
    "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
    "ax[1].imshow(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = Path('testcbir/Test_Images')\n",
    "testdf = pd.DataFrame()\n",
    "\n",
    "testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n",
    "testdf['image'] = 'testcbir/Test_Images/' + testdf['image'].astype(str)\n",
    "\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimages = testdf.image.values\n",
    "test_latent_features = get_latent_features(testimages, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_latent_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
    "MAX_RESULTS = 10\n",
    "queryIdx = 12\n",
    "\n",
    "queryFeatures = test_latent_features[queryIdx]\n",
    "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
    "imgs = []\n",
    "\n",
    "# loop over the results\n",
    "for (d, j) in results:\n",
    "    img = np.array(Image.open(images[j]))\n",
    "    print(j)\n",
    "    imgs.append(img)\n",
    "\n",
    "# display the query image\n",
    "ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n",
    "\n",
    "# build a montage from the results and display it\n",
    "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
    "ax[1].imshow(montage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
